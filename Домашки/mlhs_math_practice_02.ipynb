{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "В этом задании две части - теоретическая и практическая. Теорию можно набирать в латехе или просто решить на листочке, сфотографировать и отправить вместе с заполненным ноутбуком в anytask.\n",
        "\n",
        "Максимальный балл за задание - 10."
      ],
      "metadata": {
        "id": "fkPR4N-wiEvE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Часть 1 (теоретическая).\n",
        "\n",
        "## Задание 1 (**1.5 балла**).\n",
        "\n",
        "Пусть $f(X)=ln(detX), X\\in\\mathbb{R}^{n\\times n}$. Найдите производную $\\nabla_X f(X).$\n",
        "\n",
        "## Задание 2 (**2 балла**).\n",
        "\n",
        "Пусть $f(x)=x^Texp(xx^T)x, x\\in\\mathbb{R}^n,$ а $exp(B)$ - матричная экспонента, $B\\in\\mathbb{R}^{n\\times n}.$ Матричной экспонентой обозначают ряд\n",
        "\n",
        "$I_n+\\frac{B}{1!}+\\frac{B^2}{2!}+\\dots=\\sum\\limits_{k=0}^{\\infty}\\frac{B^k}{k!}.$\n",
        "\n",
        "Найдите производную $\\nabla_x f(x).$\n",
        "\n",
        "## Задание 3 (**1.5 балла**).\n",
        "\n",
        "В случае одномерной Ridge-регрессии минимизируется функция со штрафом:\n",
        "$Q(w) = (y-xw)^T(y-xw)+\\lambda w^2,$\n",
        "где $\\lambda$ - положительный параметр, штрафующий функцию за слишком большие значения $w$.\n",
        "\n",
        "1)  (**0.5 балла**) Найдите производную $\\nabla_w Q(w)$, выведите формулу для оптимального $w$.\n",
        "\n",
        "2) (**0.5 балла**) Найдите вторую производную $\\nabla^2_w Q(w)$. Убедитесь, что мы оказались в точке минимума.\n",
        "\n",
        "3) (**0.5 балла**) Выпишите шаг градиентного спуска в матричном виде."
      ],
      "metadata": {
        "id": "EMZ5My7kiPY_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Часть 2 (практическая).\n",
        "\n",
        "## Задание 4 (**1 балл**).\n",
        "Напишите функцию, вычисляющую значение весов в линейной регрессии по точной (аналитически найденной) формуле."
      ],
      "metadata": {
        "id": "lm6_Ln0GoliG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ols_solution(X, y):\n",
        "    # your code here"
      ],
      "metadata": {
        "id": "ZrVvpU9miOga"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Задание 5 (**1 балл**).\n",
        "Модифицируйте метод градиентного спуска с семинара так, чтобы это теперь был метод стохастического градиентного спуска."
      ],
      "metadata": {
        "id": "_shCsTQ1pVcU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def stochastic_gradient_descent(X, y, learning_rate, iterations):\n",
        "    # your code here"
      ],
      "metadata": {
        "id": "fTZWxz1zpb9R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Задание 6 (**3 балла**).\n",
        "* **(0 баллов)**. Скопируйте метод градиентного спуска из семинара в этот ноутбук.\n",
        "\n",
        "* **(0.5 балла)**. Обучите линейную регрессию тремя методами (по точной формуле, с помощью GD и с помощью SGD) на данных для задачи регрессии (см. код). Для GD и SGD используйте learning_rate = 0.01, iterations=10000.\n",
        "\n",
        "* **(0.5 балла)**. С помощью каждого метода сделайте предсказание (на всех данных), вычислите качество предсказания r2 (from sklearn.metrics import r2_score). Для получения предсказания можете использовать функцию predict с семинара.\n",
        "\n",
        "\n",
        "Ответьте на следующие вопросы (каждый вопрос - **0.5 балла**): \n",
        "\n",
        "1) все ли методы справились с нахождением минимума? если нет, то почему какой-то из методов не справился?\n",
        "\n",
        "2) сравните время работы методов (используйте библиотеку time): замеряйте время работы соответствующей написанной вами функции.\n",
        "\n",
        "3) для методов GD и SGD нарисуйте графики (для каждого свой) зависимости ошибки (loss) от номера итерации. \n",
        "\n",
        "4) какой метод успешнее всего справился с задачей? (т.е. r2 наибольший)."
      ],
      "metadata": {
        "id": "WnRlUa9Npi9o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_regression\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "X, y, _ = make_regression(n_samples=100000,#number of samples\n",
        "                          n_features=10,#number of features\n",
        "                          n_informative=8,#number of useful features \n",
        "                          noise=100,#bias and standard deviation of the guassian noise\n",
        "                          coef=True,#true coefficient used to generated the data\n",
        "                          random_state=123) \n",
        "\n",
        "X = pd.DataFrame(data=X, columns=np.arange(0, X.shape[1]))\n",
        "X[10] = X[6] + X[7] + np.random.random()*0.01"
      ],
      "metadata": {
        "id": "LBu41KSpqbbI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# your code here"
      ],
      "metadata": {
        "id": "f1SE0-oUtVlO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "your text here"
      ],
      "metadata": {
        "id": "2zrdVgQrtcAn"
      }
    }
  ]
}